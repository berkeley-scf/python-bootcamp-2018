{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Python bootcamp project \n",
    "...\n",
    "\n",
    "Background\n",
    "============\n",
    "\n",
    "The goal of the project is to analyze the tweeting behavior of the 100 US Senators in the US Senate. Each senator has a Twitter account and puts out tweets reflecting their thought and communicating with the people in the state they represent.\n",
    "\n",
    "(Note that these tweets were downloaded early in 2017, so they reflect tweeting just before President Obama stepped down and President Trump began his term.)\n",
    "\n",
    "The goal of the project questions is to guide you through the steps of getting the data, processing and cleaning it, putting it in a format that makes it easier to analyze and then doing some basic analysis. The last few questions ask you to see if whether a senator mentions a president or presidential candidate depends on the party that the senator is part of. For example, do Democratic senators mention Barack Obama in their tweets more or less than Republican senators? \n",
    "\n",
    "The file **fetch_senator_tweets.py** downloads tweets using the Python twitter package to interact with Twitter's API. You will only be able to run that code if you set up your own Twitter account and follow the instructions at the start of the file regarding filling in the authentication information (CONSUMER_KEY, CONSUMER_SECRET, etc.).\n",
    "\n",
    "I've already run the code and downloaded the data for you.  The downloaded information on the senators' twitter accounts is in *senators-list.json*, while the downloaded tweets are in *timelines.json*. Note that there are only 200 tweets for each senator because of limits on how many tweets can be accessed in a given request. *timelines.json* is too big to put in the Github repository. You can find it at <http://www.stat.berkeley.edu/~paciorek/transfer/timelines.json>.\n",
    "\n",
    "Questions\n",
    "===========\n",
    "\n",
    "1. Load the *senators-list.json* and *timelines.json* files as objects called *senators* and *timelines*.\n",
    "\n",
    "2. What type of datastructure is *timelines*? How many timelines are there? What does each timeline correspond to?\n",
    "\n",
    "3. Make a list of the number of followers each senator has.\n",
    "\n",
    "4. What is the screen name of the senator with the largest number of followers.\n",
    "\n",
    "5. Make a list of lists where the outer list represents senators and the inner list contains the text of each senator's tweets, and call it *tweets*.\n",
    "\n",
    "6. Write a function, called *remove_punct*, that takes a word and returns the word with all punctuation characters removed, except for those that occur within a word.\n",
    "\n",
    "7. Write a function that takes tweet and returns a cleaned up version of the tweet. Here is an example function to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` {.sourceCode .bash}\n",
    "def clean(tweet):\n",
    "    cleaned_words = [word.lower() for word in tweet.split() if\n",
    "                 'http' not in word and\n",
    "                 word.isalpha() and\n",
    "                 word != 'RT']\n",
    "    return ' '.join(cleaned_words)\n",
    "                                                                           \n",
    "clean(tweets[0][0])\n",
    "```\n",
    "\n",
    "Note that the function I've provided is a bit buggy - it has some problems with some tweets. If your goal is to convert the tweet into a discrete set of words, what is going wrong here? Fix up and extend the example function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use the following file to create a list, called *stopwords*, that contains common english words.  <http://www.textfixer.com/resources/common-english-words.txt>.\n",
    "Make sure to pull the data into Python by writing Python code to download and suck the data into Python.\n",
    "\n",
    "9. Write a function, called *tokenize*, which takes a tweet, cleans it, and removes all punctuation and stopwords.\n",
    "\n",
    "10. Create a list of lists, tweets_content, using your *tokenize* function.\n",
    "\n",
    "11. Create a list, *tokens*, where all 200 of each senator's tweets are made into a single string.\n",
    "\n",
    "12. Create a Pandas dataFrame with the following columns: senator name or handle, party of the senator, and number of times a prominent politician is mentioned in each senator's tweets. You might count the number of 'Obama', 'Trump', or 'Clinton' references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can use this to create the party column (1=Republican, 0=Democratic):\n",
    "``` {.sourceCode .bash}\n",
    "party = np.array([1,1,1,1,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,0,\n",
    "1,1,1,0,0,0,0,0,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,1,\n",
    "1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1])\n",
    "```\n",
    "\n",
    "That should correspond to the ordering in `timelines` but of course it would be more robust to create a dataFrame that has user names and party as columns and merge that with the count information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Use a Poisson GLM to assess the relationship between party and number of Obama/Trump/Clinton mentions. Does one party tend to mention Obama/Trump/Clinton more in their tweets? Can you deduce a pattern by considering the party of the senator and the party of Obama/Trump/Clinton?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's some syntax to help you get started:\n",
    "``` {.sourceCode .bash}\n",
    "import statsmodels.api as sm\n",
    "model = sm.GLM(endog = .,  exog = ., family = sm.families.Poisson())\n",
    "model.fit()\n",
    "```\n",
    "\n",
    "Does the statistical result make sense in light of the number of total mentions of Obama/Trump/Clinton by Republicans and the number of total mentions by Democrats?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Use *matplotlib* to make histograms of the number of Obama mentions by senator, stratified by party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is this consistent with the results of your statistical analysis?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
